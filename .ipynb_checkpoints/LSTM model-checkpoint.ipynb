{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b9893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faee11c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>S1_Temp</th>\n",
       "      <th>S2_Temp</th>\n",
       "      <th>S3_Temp</th>\n",
       "      <th>S4_Temp</th>\n",
       "      <th>S1_Light</th>\n",
       "      <th>S2_Light</th>\n",
       "      <th>S3_Light</th>\n",
       "      <th>S4_Light</th>\n",
       "      <th>S1_Sound</th>\n",
       "      <th>S2_Sound</th>\n",
       "      <th>S3_Sound</th>\n",
       "      <th>S4_Sound</th>\n",
       "      <th>S5_CO2</th>\n",
       "      <th>S5_CO2_Slope</th>\n",
       "      <th>S6_PIR</th>\n",
       "      <th>S7_PIR</th>\n",
       "      <th>Room_Occupancy_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:49:41</td>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.38</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:50:12</td>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>33</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:50:42</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.50</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:51:13</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>390</td>\n",
       "      <td>0.388462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:51:44</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:52:14</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>390</td>\n",
       "      <td>0.165385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:52:45</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>120</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:53:15</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>41</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.011538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:53:46</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.50</td>\n",
       "      <td>122</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:54:17</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.50</td>\n",
       "      <td>101</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.39</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.188462</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:54:47</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>122</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.21</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.276923</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:55:18</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.50</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.365385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:55:49</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.50</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.453846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:56:19</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.542308</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:56:50</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.630769</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:57:21</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.719231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:57:51</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>122</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.711538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:58:22</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>122</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.653846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:58:52</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.81</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.542308</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:59:23</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.50</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.373077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:59:54</td>\n",
       "      <td>25.13</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.50</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>-0.238462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>11:00:24</td>\n",
       "      <td>25.06</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.092308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>11:00:55</td>\n",
       "      <td>25.13</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>395</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>11:01:26</td>\n",
       "      <td>25.13</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.56</td>\n",
       "      <td>123</td>\n",
       "      <td>35</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>400</td>\n",
       "      <td>0.319231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>11:01:56</td>\n",
       "      <td>25.13</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.63</td>\n",
       "      <td>25.50</td>\n",
       "      <td>124</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>400</td>\n",
       "      <td>0.334615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time  S1_Temp  S2_Temp  S3_Temp  S4_Temp  S1_Light  \\\n",
       "0   2017/12/22  10:49:41    24.94    24.75    24.56    25.38       121   \n",
       "1   2017/12/22  10:50:12    24.94    24.75    24.56    25.44       121   \n",
       "2   2017/12/22  10:50:42    25.00    24.75    24.50    25.44       121   \n",
       "3   2017/12/22  10:51:13    25.00    24.75    24.56    25.44       121   \n",
       "4   2017/12/22  10:51:44    25.00    24.75    24.56    25.44       121   \n",
       "5   2017/12/22  10:52:14    25.00    24.81    24.56    25.44       121   \n",
       "6   2017/12/22  10:52:45    25.00    24.75    24.56    25.44       120   \n",
       "7   2017/12/22  10:53:15    25.00    24.81    24.56    25.44       121   \n",
       "8   2017/12/22  10:53:46    25.00    24.81    24.56    25.50       122   \n",
       "9   2017/12/22  10:54:17    25.00    24.81    24.56    25.50       101   \n",
       "10  2017/12/22  10:54:47    25.06    24.81    24.56    25.44       122   \n",
       "11  2017/12/22  10:55:18    25.06    24.81    24.56    25.50       123   \n",
       "12  2017/12/22  10:55:49    25.06    24.88    24.63    25.50       123   \n",
       "13  2017/12/22  10:56:19    25.06    24.81    24.63    25.56       123   \n",
       "14  2017/12/22  10:56:50    25.06    24.88    24.63    25.56       123   \n",
       "15  2017/12/22  10:57:21    25.06    24.88    24.63    25.56       123   \n",
       "16  2017/12/22  10:57:51    25.06    24.88    24.63    25.56       122   \n",
       "17  2017/12/22  10:58:22    25.06    24.81    24.63    25.56       122   \n",
       "18  2017/12/22  10:58:52    25.06    24.81    24.63    25.56       123   \n",
       "19  2017/12/22  10:59:23    25.06    24.88    24.63    25.50       123   \n",
       "20  2017/12/22  10:59:54    25.13    24.88    24.63    25.50       123   \n",
       "21  2017/12/22  11:00:24    25.06    24.88    24.63    25.56       123   \n",
       "22  2017/12/22  11:00:55    25.13    24.88    24.63    25.56       123   \n",
       "23  2017/12/22  11:01:26    25.13    24.88    24.63    25.56       123   \n",
       "24  2017/12/22  11:01:56    25.13    24.88    24.63    25.50       124   \n",
       "\n",
       "    S2_Light  S3_Light  S4_Light  S1_Sound  S2_Sound  S3_Sound  S4_Sound  \\\n",
       "0         34        53        40      0.08      0.19      0.06      0.06   \n",
       "1         33        53        40      0.93      0.05      0.06      0.06   \n",
       "2         34        53        40      0.43      0.11      0.08      0.06   \n",
       "3         34        53        40      0.41      0.10      0.10      0.09   \n",
       "4         34        54        40      0.18      0.06      0.06      0.06   \n",
       "5         34        54        40      0.13      0.06      0.06      0.07   \n",
       "6         34        54        40      1.39      0.32      0.43      0.06   \n",
       "7         34        54        41      0.09      0.06      0.09      0.05   \n",
       "8         35        56        43      0.09      0.05      0.06      0.13   \n",
       "9         34        57        43      3.84      0.64      0.48      0.39   \n",
       "10        35        57        43      2.20      0.31      0.33      0.21   \n",
       "11        35        57        44      0.42      0.13      0.14      0.09   \n",
       "12        35        57        43      0.21      0.15      0.07      0.06   \n",
       "13        35        57        44      1.66      0.21      0.12      0.09   \n",
       "14        35        58        44      0.57      0.17      0.21      0.13   \n",
       "15        35        58        44      0.14      0.17      0.15      0.06   \n",
       "16        35        58        44      0.24      0.05      0.10      0.05   \n",
       "17        35        57        44      0.25      0.07      0.07      0.05   \n",
       "18        35        58        44      0.23      0.07      0.08      0.06   \n",
       "19        35        58        45      0.13      0.06      0.06      0.06   \n",
       "20        35        58        44      0.13      0.04      0.06      0.06   \n",
       "21        35        57        44      0.15      0.05      0.06      0.06   \n",
       "22        35        58        45      0.11      0.05      0.06      0.06   \n",
       "23        35        58        45      1.31      0.12      0.08      0.07   \n",
       "24        35        59        45      0.11      0.12      0.06      0.06   \n",
       "\n",
       "    S5_CO2  S5_CO2_Slope  S6_PIR  S7_PIR  Room_Occupancy_Count  \n",
       "0      390      0.769231       0       0                     1  \n",
       "1      390      0.646154       0       0                     1  \n",
       "2      390      0.519231       0       0                     1  \n",
       "3      390      0.388462       0       0                     1  \n",
       "4      390      0.253846       0       0                     1  \n",
       "5      390      0.165385       0       0                     1  \n",
       "6      390      0.076923       1       0                     1  \n",
       "7      390     -0.011538       0       0                     1  \n",
       "8      390     -0.100000       0       0                     1  \n",
       "9      390     -0.188462       1       1                     1  \n",
       "10     390     -0.276923       1       1                     1  \n",
       "11     390     -0.365385       1       0                     1  \n",
       "12     390     -0.453846       1       0                     1  \n",
       "13     390     -0.542308       1       0                     1  \n",
       "14     390     -0.630769       1       0                     1  \n",
       "15     390     -0.719231       1       0                     1  \n",
       "16     395     -0.711538       0       0                     1  \n",
       "17     395     -0.653846       1       0                     1  \n",
       "18     395     -0.542308       1       0                     1  \n",
       "19     395     -0.373077       0       0                     1  \n",
       "20     390     -0.238462       0       0                     1  \n",
       "21     395     -0.092308       0       0                     1  \n",
       "22     395      0.061538       0       0                     1  \n",
       "23     400      0.319231       0       0                     1  \n",
       "24     400      0.334615       0       0                     1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Occupancy_Estimation.csv')\n",
    "columns_names = list(data.columns)\n",
    "data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7698182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16c8a255430>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data['Room_Occupancy_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f43c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1_Temp</th>\n",
       "      <th>S2_Temp</th>\n",
       "      <th>S3_Temp</th>\n",
       "      <th>S4_Temp</th>\n",
       "      <th>S1_Light</th>\n",
       "      <th>S2_Light</th>\n",
       "      <th>S3_Light</th>\n",
       "      <th>S4_Light</th>\n",
       "      <th>S1_Sound</th>\n",
       "      <th>S2_Sound</th>\n",
       "      <th>S3_Sound</th>\n",
       "      <th>S4_Sound</th>\n",
       "      <th>S5_CO2</th>\n",
       "      <th>S5_CO2_Slope</th>\n",
       "      <th>S6_PIR</th>\n",
       "      <th>S7_PIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.38</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>33</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.50</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>390</td>\n",
       "      <td>0.388462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>25.25</td>\n",
       "      <td>25.31</td>\n",
       "      <td>24.75</td>\n",
       "      <td>25.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>25.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>24.75</td>\n",
       "      <td>25.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>25.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>24.75</td>\n",
       "      <td>25.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>25.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>24.75</td>\n",
       "      <td>25.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>25.25</td>\n",
       "      <td>25.25</td>\n",
       "      <td>24.75</td>\n",
       "      <td>25.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7596 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      S1_Temp  S2_Temp  S3_Temp  S4_Temp  S1_Light  S2_Light  S3_Light  \\\n",
       "0       24.94    24.75    24.56    25.38       121        34        53   \n",
       "1       24.94    24.75    24.56    25.44       121        33        53   \n",
       "2       25.00    24.75    24.50    25.44       121        34        53   \n",
       "3       25.00    24.75    24.56    25.44       121        34        53   \n",
       "4       25.00    24.75    24.56    25.44       121        34        54   \n",
       "...       ...      ...      ...      ...       ...       ...       ...   \n",
       "7591    25.25    25.31    24.75    25.63         0         0         0   \n",
       "7592    25.25    25.25    24.75    25.63         0         0         0   \n",
       "7593    25.25    25.25    24.75    25.63         0         0         0   \n",
       "7594    25.25    25.25    24.75    25.56         0         0         0   \n",
       "7595    25.25    25.25    24.75    25.63         0         0         0   \n",
       "\n",
       "      S4_Light  S1_Sound  S2_Sound  S3_Sound  S4_Sound  S5_CO2  S5_CO2_Slope  \\\n",
       "0           40      0.08      0.19      0.06      0.06     390      0.769231   \n",
       "1           40      0.93      0.05      0.06      0.06     390      0.646154   \n",
       "2           40      0.43      0.11      0.08      0.06     390      0.519231   \n",
       "3           40      0.41      0.10      0.10      0.09     390      0.388462   \n",
       "4           40      0.18      0.06      0.06      0.06     390      0.253846   \n",
       "...        ...       ...       ...       ...       ...     ...           ...   \n",
       "7591         0      0.08      0.04      0.06      0.10     355      0.000000   \n",
       "7592         0      0.08      0.06      0.06      0.09     355      0.000000   \n",
       "7593         0      0.08      0.06      0.06      0.10     355      0.000000   \n",
       "7594         0      0.08      0.05      0.06      0.10     355      0.000000   \n",
       "7595         0      0.08      0.05      0.07      0.10     355      0.000000   \n",
       "\n",
       "      S6_PIR  S7_PIR  \n",
       "0          0       0  \n",
       "1          0       0  \n",
       "2          0       0  \n",
       "3          0       0  \n",
       "4          0       0  \n",
       "...      ...     ...  \n",
       "7591       0       0  \n",
       "7592       0       0  \n",
       "7593       0       0  \n",
       "7594       0       0  \n",
       "7595       0       0  \n",
       "\n",
       "[7596 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = data[:int(len(data)*0.75)], data[int(len(data)*0.75):]\n",
    "X_train, X_test = train_data[columns_names[2:len(columns_names)-1]], test_data[columns_names[2:len(columns_names)-1]]\n",
    "Y_train, Y_test = train_data[columns_names[len(columns_names)-1]], test_data[columns_names[len(columns_names)-1]] \n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92d8cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sequences(sequences, n_steps):    \n",
    "    X, y = [],[]\n",
    "    for i in range(len(sequences[0])):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequences[0]):\n",
    "            break\n",
    "        \n",
    "        X.append(sequences[0].iloc[i:end_ix, : ])\n",
    "        y.append(sequences[1].iloc[end_ix-1])\n",
    "        \n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "\n",
    "X,Y = split_sequences((X_train, Y_train), 3) \n",
    "X.size()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b397e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4940e+01, 2.4750e+01, 2.4560e+01, 2.5380e+01, 1.2100e+02, 3.4000e+01,\n",
       "         5.3000e+01, 4.0000e+01, 8.0000e-02, 1.9000e-01, 6.0000e-02, 6.0000e-02,\n",
       "         3.9000e+02, 7.6923e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [2.4940e+01, 2.4750e+01, 2.4560e+01, 2.5440e+01, 1.2100e+02, 3.3000e+01,\n",
       "         5.3000e+01, 4.0000e+01, 9.3000e-01, 5.0000e-02, 6.0000e-02, 6.0000e-02,\n",
       "         3.9000e+02, 6.4615e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [2.5000e+01, 2.4750e+01, 2.4500e+01, 2.5440e+01, 1.2100e+02, 3.4000e+01,\n",
       "         5.3000e+01, 4.0000e+01, 4.3000e-01, 1.1000e-01, 8.0000e-02, 6.0000e-02,\n",
       "         3.9000e+02, 5.1923e-01, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define hyperparameters\n",
    "batchsize = 100\n",
    "sequence_len = X.size()[1]\n",
    "input_len = X.size()[2]\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a05bce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_len, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden == None:\n",
    "            self.hidden = (torch.zeros(1,1,self.hidden_size), torch.zeros(1,1,self.hidden_size))\n",
    "        else:\n",
    "            self.hidden = hidden\n",
    "            \n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(x),1,-1), self.hidden)\n",
    "        predictions = self.linear(lstm_out.view(len(x), -1))\n",
    "        \n",
    "        return predictions[-1], self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea84174b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[1;32m----> 9\u001b[0m         y_pred, _ \u001b[38;5;241m=\u001b[39m model(x, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [27], line 15\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m=\u001b[39m hidden\n\u001b[1;32m---> 15\u001b[0m lstm_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(lstm_out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_size=input_len, hidden_size=50, output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 600\n",
    "model.train()\n",
    "for epoch in range(epochs + 1):\n",
    "    for x,y in zip(X, Y):\n",
    "        y_pred, _ = model(x, None)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch: {epoch:4} loss:{loss.item():10.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fe027ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4940e+01, 2.4750e+01, 2.4560e+01, 2.5380e+01, 1.2100e+02, 3.4000e+01,\n",
       "         5.3000e+01, 4.0000e+01, 8.0000e-02, 1.9000e-01, 6.0000e-02, 6.0000e-02,\n",
       "         3.9000e+02, 7.6923e-01, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2, Y2 = split_sequences((X_train, Y_train), 1)\n",
    "X2.size()[2]\n",
    "X2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a53b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, sample_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sample_size = sample_size\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(sample_size, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b306273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = NeuralNetwork(X2.size()[2]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1385a125",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predicted_num \u001b[38;5;241m=\u001b[39m model(X2[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m predicted_num\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [37], line 15\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_relu_stack(x)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Python 3.9.9\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "predicted_num = model(X2[0][0])\n",
    "predicted_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
